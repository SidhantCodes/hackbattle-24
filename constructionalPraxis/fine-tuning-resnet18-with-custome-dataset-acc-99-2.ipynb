{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets try this with Classification problem\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "resnet18_model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(28, 28)),\n",
    "    transforms.RandomHorizontalFlip(p=0.7),\n",
    "    transforms.RandomRotation(10),  # Added rotation\n",
    "    transforms.ToTensor(),\n",
    " \n",
    "\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 3228\n",
       "    Root location: C:\\Users\\86195\\OneDrive\\Desktop\\HuiHui\\trainpy\\shapes_dataset\\train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(28, 28), interpolation=bilinear, max_size=None, antialias=True)\n",
       "               RandomHorizontalFlip(p=0.7)\n",
       "               RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = datasets.ImageFolder(root=r\"C:\\Users\\86195\\OneDrive\\Desktop\\HuiHui\\trainpy\\shapes_dataset\\train\",\n",
    "                                  transform=data_transform, # a transform for the data\n",
    "                                  target_transform=None) # a transform for the label/target \n",
    "\n",
    "# test_data = datasets.ImageFolder(root=\"/kaggle/input/geometric-shapes-mathematics/dataset/test\",\n",
    "#                                  transform=data_transform)\n",
    "\n",
    "\n",
    "# validation_data = datasets.ImageFolder(root=\"/kaggle/input/geometric-shapes-mathematics/dataset/val\",\n",
    "#                                  transform=data_transform)\n",
    "# train_data, test_data ,validation_data\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0., 0., 0.,  ..., 1., 1., 0.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "          ...,\n",
       "          [0., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [0., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [0., 1., 1.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 1., 1., 0.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "          ...,\n",
       "          [0., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [0., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [0., 1., 1.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 1., 1., 0.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "          ...,\n",
       "          [0., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [0., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [0., 1., 1.,  ..., 0., 0., 0.]]]),\n",
       " 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cube', 'Overlapping-Rectangle', 'circle', 'rhombus']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get class names as dict\n",
    "class_names = train_data.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cube': 0, 'Overlapping-Rectangle': 1, 'circle': 2, 'rhombus': 3}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get class names as dict\n",
    "class_dict = train_data.class_to_idx\n",
    "class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3228"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the lengths of our dataset\n",
    "# len(train_data), len(test_data)\n",
    "len(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.samples[11111]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image tensor:\n",
      " tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])\n",
      "Image shape: torch.Size([3, 28, 28])\n",
      "Image datatype: torch.float32\n",
      "Image label: 0\n",
      "Label datatype: <class 'int'>\n",
      "class name: Cube\n"
     ]
    }
   ],
   "source": [
    "# Index on the train_data Dataset to get a single image and label\n",
    "img, label = train_data[100][0], train_data[100][1]\n",
    "print(f\"Image tensor:\\n {img}\")\n",
    "print(f\"Image shape: {img.shape}\")\n",
    "print(f\"Image datatype: {img.dtype}\")\n",
    "print(f\"Image label: {label}\")\n",
    "print(f\"Label datatype: {type(label)}\")\n",
    "print(f\"class name: {class_names[label]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([3, 28, 28]) -> [color_channels, height, width]\n",
      "Image permute: torch.Size([28, 28, 3]) -> [height, width, color_channels]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Cube')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random \n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "img_permute = img.permute(1, 2, 0)\n",
    "\n",
    "# Print out different shapes\n",
    "print(f\"Original shape: {img.shape} -> [color_channels, height, width]\")\n",
    "print(f\"Image permute: {img_permute.shape} -> [height, width, color_channels]\")\n",
    "\n",
    "# Plot \n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.imshow(img_permute)\n",
    "plt.axis(\"off\")\n",
    "plt.title(class_names[label], fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x13ae6a1be90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE=32\n",
    "train_dataloader = DataLoader(dataset=train_data,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              num_workers=1,\n",
    "                              shuffle=True)\n",
    "\n",
    "# test_dataloader = DataLoader(dataset=test_data,\n",
    "#                              batch_size=BATCH_SIZE,\n",
    "#                              num_workers=1,\n",
    "#                              shuffle=False)\n",
    "\n",
    "# train_dataloader, test_dataloader\n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([32, 3, 28, 28]) -> [batch_size, color_channels, height, width]\n",
      "Label shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "img, label = next(iter(train_dataloader))\n",
    "\n",
    "# Batch size will now be 1, you can change the batch size if you like\n",
    "print(f\"Image shape: {img.shape} -> [batch_size, color_channels, height, width]\")\n",
    "print(f\"Label shape: {label.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18_model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of learnable parameters: 11689512\n"
     ]
    }
   ],
   "source": [
    "# Count the number of learnable parameters\n",
    "total_params = sum(p.numel() for p in resnet18_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Total number of learnable parameters:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (1): Dropout(p=0.2, inplace=False)\n",
       "  (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (3): Linear(in_features=256, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(resnet18_model.fc.out_features) # -> print the out features of the resnet 18 model -> 1000 neuron\n",
    "num_features = resnet18_model.fc.in_features # this is getting the 512 neurons\n",
    "num_classes = 8\n",
    "# now lets make an trainable mlp layer\n",
    "resnet18_model.fc = nn.Sequential (\n",
    "    nn.Linear(num_features,512),\n",
    "    nn.Dropout(0.2),\n",
    "     nn.Linear(512,256),\n",
    "    nn.Linear(256,num_classes), # -> len(labels) labels according to labels\n",
    ")\n",
    "\n",
    "# we will take the fc layer after modification and print it out\n",
    "resnet18_model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'circle')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGzCAYAAABpdMNsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAgT0lEQVR4nO3df3BU9f3v8dcmJAtIshhCfkGgARUsgfRbhMioiF8yQPotF4TOiD+uYPnCiIErUqvFUcH220ZxrnW0qOPcUeodUUsRGL2VFgIJYxuwgFwu05oLmbSgJOGHzW4IJETyuX9w3W9XAuQsu3kn4fmYOTPZ3fM5570nJ/vK2XP2vT7nnBMAAJ0swboAAMDViQACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAJioLy8XD6fT+Xl5TFbps/n08qVK2O2PKCrIYAAACZ6WRcA9AQTJ07UmTNnlJycbF0K0G1wBATEQEJCgnr37q2EhEv/SZ0+fbqTKgK6PgII6KAvvvhC8+fPV05Ojvx+v/Ly8rRo0SKdPXu23XNAkyZNUn5+vvbs2aOJEyeqb9++euKJJyRJzc3NWrlypW644Qb17t1b2dnZmjVrlqqrqy9bww9/+ENlZmbK7/dr1KhReuONN+L5tIG44S04oAOOHj2q8ePHq6GhQQsXLtTIkSP1xRdf6Le//e0lj2pOnjyp4uJizZkzR/fdd58yMzN17tw5ff/731dZWZnmzJmjhx9+WI2NjdqyZYsOHDig4cOHt7us+vp63XzzzfL5fFq8eLEGDhyojz76SPPnz1coFNLSpUvj9OyBOHEALuv+++93CQkJ7s9//vMFj7W1tbnt27c7SW779u3h+2+//XYnyb322msR87/xxhtOknvhhRfaXdbXJLkVK1aEb8+fP99lZ2e7EydORIyZM2eOCwQC7vTp01E+O8AGb8EBl9HW1qaNGzdq+vTpuummmy543OfzXXSs3+/XAw88EHHf+vXrlZ6eriVLlnR4Wc45rV+/XtOnT5dzTidOnAhPU6dOVTAY1N69ez0+M8AWb8EBl3H8+HGFQiHl5+d7Hjto0KALroyrrq7WiBEj1KtXx//8jh8/roaGBr3++ut6/fXX253n2LFjnusDLBFAQBz16dMnJstpa2uTJN13332aO3duu/OMGTMmJusCOgsBBFzGwIEDlZqaqgMHDsRkecOHD9euXbvU2tqqpKSkDteQkpKic+fOqaioKCZ1ANY4BwRcRkJCgmbOnKkPPvhAu3fvvuBx55yn5c2ePVsnTpzQr371qw4vKzExUbNnz9b69evbDcLjx497qgHoCjgCAjrgF7/4hf7whz/o9ttv18KFC3XjjTeqtrZW69at08cff+xpWffff7/eeustLVu2TJ988oluu+02NTU1aevWrXrooYc0Y8aMdsc9++yz2r59uwoLC7VgwQJ9+9vf1pdffqm9e/dq69at+vLLL2PxVIFOQwABHTBo0CDt2rVLTz31lN5++22FQiENGjRIxcXF6tu3r6dlJSYm6ne/+51+/vOfa+3atVq/fr0GDBigW2+9VaNHj77ouMzMTH3yySf66U9/qvfff1+vvPKKBgwYoFGjRum555670qcIdDqf8/r+AQAAMcA5IACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgost9DqitrU1Hjx5VSkrKJbsMAwC6JuecGhsblZOTc8lvCe5yAXT06FHl5uZalwEAuEJHjhzR4MGDL/p4lwuglJQU6xIQR8Fg0LoEAB4FAoGoxl3u9TxuAbR69Wo9//zzqqurU0FBgV5++WWNHz/+suN4261nS01NtS4BQCe53Ot5XC5CeO+997Rs2TKtWLFCe/fuVUFBgaZOncoXZgEAwuLSC66wsFDjxo0Lt5tva2tTbm6ulixZop/85CeXHBsKhaI+3EPXR+tBoPuJ9p2pYDB4yXc9Yn4EdPbsWe3ZsyfiS7MSEhJUVFSkysrKC+ZvaWlRKBSKmAAAPV/MA+jEiRM6d+6cMjMzI+7PzMxUXV3dBfOXlpYqEAiEJ66AA4Crg/kHUZcvX65gMBiejhw5Yl0SAKATxPwquPT0dCUmJqq+vj7i/vr6emVlZV0wv9/vl9/vj3UZAIAuLuZHQMnJyRo7dqzKysrC97W1tamsrEwTJkyI9eoAAN1UXD4HtGzZMs2dO1c33XSTxo8frxdffFFNTU164IEH4rE6AEA3FJcAuuuuu3T8+HE9/fTTqqur03e+8x1t3rz5ggsTAABXr7h8DuhKfP05oMtdPw4A6Bzd5nNAAAB0BAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEzEPIBWrlwpn88XMY0cOTLWqwEAdHO94rHQUaNGaevWrf+5kl5xWQ0AoBuLSzL06tVLWVlZ8Vg0AKCHiMs5oIMHDyonJ0fDhg3Tvffeq8OHD1903paWFoVCoYgJANDzxTyACgsLtWbNGm3evFmvvvqqampqdNttt6mxsbHd+UtLSxUIBMJTbm5urEsCAHRBPueci+cKGhoaNHToUL3wwguaP3/+BY+3tLSopaUlfDsUCik3N1fBYFCpqanxLA0A0AE+ny+qcZd7HY/71QH9+/fXDTfcoEOHDrX7uN/vl9/vj3cZAIAuJu6fAzp16pSqq6uVnZ0d71UBALqRmAfQo48+qoqKCv3tb3/Tn/70J915551KTEzU3XffHetVAQC6sZi/Bff555/r7rvv1smTJzVw4EDdeuut2rlzpwYOHBjrVQEAurGYB9C7774b60Xiquf9OpnoL62J6zU5/ySKk7pRDYnu5DHQGegFBwAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwETcv5AO+GdnT3lv9vlVc5vnMS0N3sdI0j8OeR83MN/7/3GnvvC+HfpkeG8smjIo0fMYKbpvwGw69pXnMf2yo6gvwXttNGXtmjgCAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYoBs2oue9obP+vKrZ85iEJO/ryZkQXffjY3+Ookv1AO/r+ext7123A9d7r63fYO8dqiXpxP/2vq68f/P+clKzucXzmGHf975D9MuIriu4ougKjo7jCAgAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJn3MuipaS8RMKhRQIBBQMBpWammpdjjHvv5pofp2tTdHtAmdPeW+oeTYU1ao8S+obXRPJvlne/yfzKYp1JXjf5ue893GVi6ZjrKTT9VE0S/2W94afocPnPI/Z92Kr5zG9UqPbH276sffGp8n9otmHetaxQEdfx3vWswYAdBsEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBM9LIuAJcSRQPFKIacPfWV90GS9vx37+Nu+lGy5zHXZHpvcilfdM0nO4/3+hL6xqGMi0jO8/6/aTSNcFOHeH8JKlzhfdud+sJ7c9Vox/Uf5n3bJXrvedojcAQEADBBAAEATHgOoB07dmj69OnKycmRz+fTxo0bIx53zunpp59Wdna2+vTpo6KiIh08eDBW9QIAegjPAdTU1KSCggKtXr263cdXrVqll156Sa+99pp27dqla665RlOnTlVzcxTfpgUA6LE8nwEsLi5WcXFxu4855/Tiiy/qySef1IwZMyRJb731ljIzM7Vx40bNmTPnyqoFAPQYMT0HVFNTo7q6OhUVFYXvCwQCKiwsVGVlZbtjWlpaFAqFIiYAQM8X0wCqq6uTJGVmZkbcn5mZGX7sm0pLSxUIBMJTbm5uLEsCAHRR5lfBLV++XMFgMDwdOXLEuiQAQCeIaQBlZWVJkurr6yPur6+vDz/2TX6/X6mpqRETAKDni2kA5eXlKSsrS2VlZeH7QqGQdu3apQkTJsRyVQCAbs7zVXCnTp3SoUOHwrdramq0b98+paWlaciQIVq6dKn+4z/+Q9dff73y8vL01FNPKScnRzNnzoxl3QCAbs5zAO3evVt33HFH+PayZcskSXPnztWaNWv02GOPqampSQsXLlRDQ4NuvfVWbd68Wb17945d1QCAbs/noukgGEehUEiBQEDBYLCHnQ/yvpm/avY+5vBW7w1C/ddGtwu0nvE+ZmiR92akvmg6rMJANPuR999tNC9ZNf+r1fMYSar67TnPYya94H0f75MWRcPdLqyjr+PmV8EBAK5OBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATnr+OAdGJpoNv7S7vna3/z2ttnseMfTy6btND/jXJ8xg6W/dkXfh367z/XUiSP8X7mKQ+Ua3qqsQREADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABM0I+0kbV95b0b65V+9N1DMnep5iHJu9t5UVJISErpw80n0YN7/ljLGJUa1ppSh3teVkMTfRUdxBAQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEzUi7sOxC7/8f9M3y3gjRF/VeQNNFdL62c97H/N93oxgkacBo73+D6dH1Pb0qcQQEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABM1IO0lbq/fGnQf+h/cGigMLvK9n1L9H93+Ij16kMHCmvs3zmNqPo1tXekF049AxHAEBAEwQQAAAE54DaMeOHZo+fbpycnLk8/m0cePGiMfnzZsnn88XMU2bNi1W9QIAegjPAdTU1KSCggKtXr36ovNMmzZNtbW14emdd965oiIBAD2P54sQiouLVVxcfMl5/H6/srKyoi4KANDzxeUcUHl5uTIyMjRixAgtWrRIJ0+evOi8LS0tCoVCERMAoOeLeQBNmzZNb731lsrKyvTcc8+poqJCxcXFOneu/UuKS0tLFQgEwlNubm6sSwIAdEEx/xzQnDlzwj+PHj1aY8aM0fDhw1VeXq7JkydfMP/y5cu1bNmy8O1QKEQIAcBVIO6XYQ8bNkzp6ek6dOhQu4/7/X6lpqZGTACAni/uAfT555/r5MmTys7OjveqAADdiOe34E6dOhVxNFNTU6N9+/YpLS1NaWlpeuaZZzR79mxlZWWpurpajz32mK677jpNnTo1poUDALo3zwG0e/du3XHHHeHbX5+/mTt3rl599VXt379fv/71r9XQ0KCcnBxNmTJFP/vZz+T3+2NXNQCg2/McQJMmTZJz7qKP//73v7+ignqqXr29d+7MnZzoeUzdnq88j/nqtOchkqTkfhffDy6m7RL7zsUk+OgY1VO1tXrfH842eR8zuMjzEEnSwO9Ese/RpbfD+MsGAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJiI+Vdyo32+BO8dcvP+LcnzmIx/8b6ev//hnOcxkpR9s/euxEnXeF9P0jXen1NCLzoSdwd7f9nieUxrs/f97l/+m/e/JUnqHfDekV5i3+sojoAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYoBlpF9bL732M/1rvjRDr90TXjLTxqPdxo/6r96aQf/mf3htWDv8v0e3aiX7v2y+pr/cxLop+lZ3Z4tK1dc56Bt3h/X/gL//ivbhTn3tvYCpJySnexyUk0oy0ozgCAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYMLnnIuuS1+chEIhBQIBz+O62NPoZqLbds0N3puRtvzD+7qaGzwPUcuX0XXT/Mch7+MaD3t/Ttf/wHuz1OOfel9Pxk3RNcZsOup9XXW7vG+7Uf/uvTltv2zv/zcnJETbIJTGotH4+nU8GAwqNTX1ovNxBAQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMCE946IwP/n75/oeUxSP+9NLgN53v9PirY57YB87w01T9d7H+OiaADbVOd9zJmT0W0HXy/vTTivvcH77yklJ4r/gX3RNAilqWhXxBEQAMAEAQQAMOEpgEpLSzVu3DilpKQoIyNDM2fOVFVVVcQ8zc3NKikp0YABA9SvXz/Nnj1b9fX1MS0aAND9eQqgiooKlZSUaOfOndqyZYtaW1s1ZcoUNTU1hed55JFH9MEHH2jdunWqqKjQ0aNHNWvWrJgXDgDo3q7oG1GPHz+ujIwMVVRUaOLEiQoGgxo4cKDWrl2rH/zgB5Kkzz77TDfeeKMqKyt18803X3aZfCOqhei2XTSj2r7yPiqxV+ddhHD6WNe9CKHmQ+/ryZrgeYgkyZ3zftK++bj39YyY4/0bUaO5CMHHRQidqlO+ETUYDEqS0tLSJEl79uxRa2urioqKwvOMHDlSQ4YMUWVlZbvLaGlpUSgUipgAAD1f1AHU1tampUuX6pZbblF+fr4kqa6uTsnJyerfv3/EvJmZmaqrq2t3OaWlpQoEAuEpNzc32pIAAN1I1AFUUlKiAwcO6N13372iApYvX65gMBiejhw5ckXLAwB0D1F9EHXx4sX68MMPtWPHDg0ePDh8f1ZWls6ePauGhoaIo6D6+nplZWW1uyy/3y+/3x9NGQCAbszTEZBzTosXL9aGDRu0bds25eXlRTw+duxYJSUlqaysLHxfVVWVDh8+rAkTojwbCgDokTwdAZWUlGjt2rXatGmTUlJSwud1AoGA+vTpo0AgoPnz52vZsmVKS0tTamqqlixZogkTJnToCjgAwNXDUwC9+uqrkqRJkyZF3P/mm29q3rx5kqRf/vKXSkhI0OzZs9XS0qKpU6fqlVdeiUmxAICe44o+BxQPfA4IlqL5fE5Un4jqpNV09Y+/+Hx0A+uJOuVzQAAARIsAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYCKqb0QFeipfVO2joxjTSasBujKOgAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACY8BVBpaanGjRunlJQUZWRkaObMmaqqqoqYZ9KkSfL5fBHTgw8+GNOiAQDdn6cAqqioUElJiXbu3KktW7aotbVVU6ZMUVNTU8R8CxYsUG1tbXhatWpVTIsGAHR/vbzMvHnz5ojba9asUUZGhvbs2aOJEyeG7+/bt6+ysrJiUyEAoEe6onNAwWBQkpSWlhZx/9tvv6309HTl5+dr+fLlOn369EWX0dLSolAoFDEBAHo+T0dA/6ytrU1Lly7VLbfcovz8/PD999xzj4YOHaqcnBzt379fjz/+uKqqqvT++++3u5zS0lI988wz0ZYBAOimfM45F83ARYsW6aOPPtLHH3+swYMHX3S+bdu2afLkyTp06JCGDx9+weMtLS1qaWkJ3w6FQsrNzfVcT5RPAwAQY6FQSIFAQMFgUKmpqRedL6ojoMWLF+vDDz/Ujh07Lhk+klRYWChJFw0gv98vv98fTRkAgG7MUwA557RkyRJt2LBB5eXlysvLu+yYffv2SZKys7OjKhAA0DN5CqCSkhKtXbtWmzZtUkpKiurq6iRJgUBAffr0UXV1tdauXavvfe97GjBggPbv369HHnlEEydO1JgxY+LyBAAA3ZOnc0A+n6/d+998803NmzdPR44c0X333acDBw6oqalJubm5uvPOO/Xkk09e8n3Af/b1e4decQ4IALqGjp4DivoihHghgACge+toANELDgBgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgIqpvRAWAWLjYV7zg6sAREADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMdLlecM65qMaFQqEYVwIAuBKXez3vcgHU2NgY1bhAIBDjSgAAV6KxsfGSr80+F+0hR5y0tbXp6NGjSklJuaBTbigUUm5uro4cOaLU1FSjCu2xHc5jO5zHdjiP7XBeV9gOzjk1NjYqJydHCQkXP9PT5Y6AEhISNHjw4EvOk5qaelXvYF9jO5zHdjiP7XAe2+E86+3QkXeluAgBAGCCAAIAmOhWAeT3+7VixQr5/X7rUkyxHc5jO5zHdjiP7XBed9oOXe4iBADA1aFbHQEBAHoOAggAYIIAAgCYIIAAACYIIACAiW4TQKtXr9a3vvUt9e7dW4WFhfrkk0+sS+p0K1eulM/ni5hGjhxpXVbc7dixQ9OnT1dOTo58Pp82btwY8bhzTk8//bSys7PVp08fFRUV6eDBgzbFxtHltsO8efMu2D+mTZtmU2yclJaWaty4cUpJSVFGRoZmzpypqqqqiHmam5tVUlKiAQMGqF+/fpo9e7bq6+uNKo6PjmyHSZMmXbA/PPjgg0YVt69bBNB7772nZcuWacWKFdq7d68KCgo0depUHTt2zLq0Tjdq1CjV1taGp48//ti6pLhrampSQUGBVq9e3e7jq1at0ksvvaTXXntNu3bt0jXXXKOpU6equbm5kyuNr8ttB0maNm1axP7xzjvvdGKF8VdRUaGSkhLt3LlTW7ZsUWtrq6ZMmaKmpqbwPI888og++OADrVu3ThUVFTp69KhmzZplWHXsdWQ7SNKCBQsi9odVq1YZVXwRrhsYP368KykpCd8+d+6cy8nJcaWlpYZVdb4VK1a4goIC6zJMSXIbNmwI325ra3NZWVnu+eefD9/X0NDg/H6/e+eddwwq7Bzf3A7OOTd37lw3Y8YMk3qsHDt2zElyFRUVzrnzv/ukpCS3bt268Dx//etfnSRXWVlpVWbcfXM7OOfc7bff7h5++GG7ojqgyx8BnT17Vnv27FFRUVH4voSEBBUVFamystKwMhsHDx5UTk6Ohg0bpnvvvVeHDx+2LslUTU2N6urqIvaPQCCgwsLCq3L/KC8vV0ZGhkaMGKFFixbp5MmT1iXFVTAYlCSlpaVJkvbs2aPW1taI/WHkyJEaMmRIj94fvrkdvvb2228rPT1d+fn5Wr58uU6fPm1R3kV1uW7Y33TixAmdO3dOmZmZEfdnZmbqs88+M6rKRmFhodasWaMRI0aotrZWzzzzjG677TYdOHBAKSkp1uWZqKurk6R294+vH7taTJs2TbNmzVJeXp6qq6v1xBNPqLi4WJWVlUpMTLQuL+ba2tq0dOlS3XLLLcrPz5d0fn9ITk5W//79I+btyftDe9tBku655x4NHTpUOTk52r9/vx5//HFVVVXp/fffN6w2UpcPIPyn4uLi8M9jxoxRYWGhhg4dqt/85jeaP3++YWXoCubMmRP+efTo0RozZoyGDx+u8vJyTZ482bCy+CgpKdGBAweuivOgl3Kx7bBw4cLwz6NHj1Z2drYmT56s6upqDR8+vLPLbFeXfwsuPT1diYmJF1zFUl9fr6ysLKOquob+/fvrhhtu0KFDh6xLMfP1PsD+caFhw4YpPT29R+4fixcv1ocffqjt27dHfH9YVlaWzp49q4aGhoj5e+r+cLHt0J7CwkJJ6lL7Q5cPoOTkZI0dO1ZlZWXh+9ra2lRWVqYJEyYYVmbv1KlTqq6uVnZ2tnUpZvLy8pSVlRWxf4RCIe3ateuq3z8+//xznTx5skftH845LV68WBs2bNC2bduUl5cX8fjYsWOVlJQUsT9UVVXp8OHDPWp/uNx2aM++ffskqWvtD9ZXQXTEu+++6/x+v1uzZo37y1/+4hYuXOj69+/v6urqrEvrVD/60Y9ceXm5q6mpcX/84x9dUVGRS09Pd8eOHbMuLa4aGxvdp59+6j799FMnyb3wwgvu008/dX//+9+dc849++yzrn///m7Tpk1u//79bsaMGS4vL8+dOXPGuPLYutR2aGxsdI8++qirrKx0NTU1buvWre673/2uu/76611zc7N16TGzaNEiFwgEXHl5uautrQ1Pp0+fDs/z4IMPuiFDhrht27a53bt3uwkTJrgJEyYYVh17l9sOhw4dcj/96U/d7t27XU1Njdu0aZMbNmyYmzhxonHlkbpFADnn3Msvv+yGDBnikpOT3fjx493OnTutS+p0d911l8vOznbJyclu0KBB7q677nKHDh2yLivutm/f7iRdMM2dO9c5d/5S7KeeesplZmY6v9/vJk+e7KqqqmyLjoNLbYfTp0+7KVOmuIEDB7qkpCQ3dOhQt2DBgh73T1p7z1+Se/PNN8PznDlzxj300EPu2muvdX379nV33nmnq62ttSs6Di63HQ4fPuwmTpzo0tLSnN/vd9ddd5378Y9/7ILBoG3h38D3AQEATHT5c0AAgJ6JAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACb+H3ZjkNUbICoPAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = train_data[1500][0], train_data[1500][1]\n",
    "img_permute = img.permute(1, 2, 0)\n",
    "\n",
    "label_name = class_names[label]\n",
    "image_per = img.permute(1,2,0)\n",
    "image_per.shape\n",
    "plt.imshow(image_per)\n",
    "plt.title(label_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(resnet18_model.parameters(), lr=1e-4)\n",
    "optimizer = torch.optim.SGD(resnet18_model.parameters(), lr=0.01, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device: torch.device = 'cpu'):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)  # output model logits\n",
    "\n",
    "        # 2. Calculate the loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy metric\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item() / len(y_pred)\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "\n",
    "    return train_loss  ,train_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a train function that takes in various model parameters + optimizer + dataloaders + loss function\n",
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader,\n",
    "          optimizer,\n",
    "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
    "          epochs: int = 5, \n",
    "          device=device):\n",
    "  \n",
    "  # 2. Create empty results dictionary\n",
    "  results = {\"train_loss\": [],\n",
    "             \"train_acc\": [],\n",
    "            }\n",
    "  \n",
    "  # 3. Loop through training and testing steps for a number of epochs\n",
    "  for epoch in tqdm(range(epochs)):\n",
    "    train_loss, train_acc = train_step(model=model,\n",
    "                                       dataloader=train_dataloader,\n",
    "                                       loss_fn=loss_fn,\n",
    "                                       optimizer=optimizer,\n",
    "                                       device=device)\n",
    "    # test_loss, test_acc = test_step(model=model,\n",
    "    #                                 dataloader=test_dataloader,\n",
    "    #                                 loss_fn=loss_fn,\n",
    "    #                                 device=device)\n",
    "    \n",
    "    # 4. Print out what's happening\n",
    "    print(f\"Epoch: {epoch} | Train loss: {train_loss:.4f} | Train acc: {train_acc:.4f}\")\n",
    "\n",
    "    # 5. Update results dictionary\n",
    "    results[\"train_loss\"].append(train_loss)\n",
    "    results[\"train_acc\"].append(train_acc)\n",
    "    # results[\"test_loss\"].append(test_loss)\n",
    "    # results[\"test_acc\"].append(test_acc)\n",
    "  \n",
    "  # 6. Return the filled results at the end of the epochs\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:05<00:51,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train loss: 0.0866 | Train acc: 0.9548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:11<00:45,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Train loss: 0.0894 | Train acc: 0.9560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:16<00:38,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Train loss: 0.0717 | Train acc: 0.9690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:22<00:32,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Train loss: 0.0480 | Train acc: 0.9773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:27<00:27,  5.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Train loss: 0.0478 | Train acc: 0.9771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:33<00:21,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Train loss: 0.0515 | Train acc: 0.9761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:38<00:16,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Train loss: 0.0417 | Train acc: 0.9848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:44<00:11,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Train loss: 0.0418 | Train acc: 0.9836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:49<00:05,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Train loss: 0.0364 | Train acc: 0.9855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:56<00:00,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Train loss: 0.0378 | Train acc: 0.9861\n",
      "Total training time: 56.719 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "epochs = 10\n",
    "resnet18_model.to(device)\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "start_time = timer() \n",
    "\n",
    "# Train model_0\n",
    "resnet18 = train(model=resnet18_model,\n",
    "                        train_dataloader=train_dataloader,\n",
    "                        # test_dataloader=test_dataloader,\n",
    "                        optimizer=optimizer,\n",
    "                        loss_fn=loss_fn,\n",
    "                        epochs=epochs)\n",
    "\n",
    "# End the timer and print out how long it took\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model: torch.nn.Module,\n",
    "                     data: list,\n",
    "                     device: torch.device = device):\n",
    "  pred_probs = []\n",
    "  model.to(device)\n",
    "  model.eval()\n",
    "  with torch.inference_mode():\n",
    "    for sample in data:\n",
    "      # Prepare the sample (add a batch dimension and pass to target device)\n",
    "      sample = torch.unsqueeze(sample, dim=0).to(device)\n",
    "\n",
    "      # Forward pass (model outputs raw logits)\n",
    "      pred_logit = model(sample)\n",
    "\n",
    "      # Get prediction probability (logit -> prediction probability)\n",
    "      pred_prob = torch.softmax(pred_logit.squeeze(), dim=0)\n",
    "\n",
    "      # Get pred_prob off the GPU for further calculations\n",
    "      pred_probs.append(pred_prob.cpu())\n",
    "\n",
    "  # Stack the pred_probs to turn list into a tensor\n",
    "  return torch.stack(pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model's state dictionary\n",
    "torch.save(resnet18_model.state_dict(), 'resnet18_pretrained-2.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
